{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d076ea64-ea9f-49d6-82d2-209da4556e78",
   "metadata": {
    "libroFormatter": "formatter-string"
   },
   "source": [
    "# 多媒体处理脚本 DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1105f-87f4-4b4c-9edf-902b503cd2b3",
   "metadata": {
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "213f0acb-ae0b-4d4e-8045-e18f101da5dc",
   "metadata": {
    "libroFormatter": "formatter-string"
   },
   "source": [
    "## Align reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05581a41-300a-4afd-9ef6-f19acb29a270",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "# reference image aligned\n",
    "import sys\n",
    "from src.utils.img_utils import pil_to_cv2, cv2_to_pil, center_crop_cv2, pils_from_video, save_videos_from_pils, save_video_from_cv2_list\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython import embed\n",
    "import numpy as np\n",
    "import copy\n",
    "from src.utils.motion_utils import motion_sync\n",
    "import pathlib\n",
    "import torch\n",
    "import pickle\n",
    "from glob import glob\n",
    "import os\n",
    "from src.models.dwpose.dwpose_detector import dwpose_detector as dwprocessor\n",
    "from src.models.dwpose.util import draw_pose\n",
    "import decord\n",
    "from tqdm import tqdm\n",
    "from moviepy.editor import AudioFileClip, VideoFileClip\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "##################################\n",
    "process_num = 100 #1266\n",
    "\n",
    "start = 0\n",
    "end = process_num + start\n",
    "#################################\n",
    "MAX_SIZE = 768\n",
    "\n",
    "def convert_fps(src_path, tgt_path, tgt_fps=24, tgt_sr=16000):\n",
    "    clip = VideoFileClip(src_path)\n",
    "    new_clip = clip.set_fps(tgt_fps)\n",
    "    if tgt_fps is not None:\n",
    "        audio = new_clip.audio\n",
    "        audio = audio.set_fps(tgt_sr)\n",
    "        new_clip = new_clip.set_audio(audio)\n",
    "    if '.mov' in tgt_path:\n",
    "        tgt_path = tgt_path.replace('.mov', '.mp4')\n",
    "    new_clip.write_videofile(tgt_path, codec='libx264', audio_codec='aac')\n",
    "    \n",
    "def get_video_pose(\n",
    "        video_path: str, \n",
    "        sample_stride: int=1,\n",
    "        max_frame=None):\n",
    "\n",
    "    # read input video\n",
    "    vr = decord.VideoReader(video_path, ctx=decord.cpu(0))\n",
    "    sample_stride *= max(1, int(vr.get_avg_fps() / 24))\n",
    "\n",
    "    frames = vr.get_batch(list(range(0, len(vr), sample_stride))).asnumpy()\n",
    "    # print(frames[0])\n",
    "    if max_frame is not None:\n",
    "        frames = frames[0:max_frame,:,:]\n",
    "    height, width, _ = frames[0].shape\n",
    "    detected_poses = [dwprocessor(frm) for frm in frames]\n",
    "    dwprocessor.release_memory()\n",
    "\n",
    "    return detected_poses, height, width, frames\n",
    "\n",
    "def save_pose_params_item(input_items):\n",
    "    detected_pose, pose_params, draw_pose_params, save_dir = input_items\n",
    "    w_min, w_max, h_min, h_max = pose_params\n",
    "    num = detected_pose['num']\n",
    "    candidate_body = detected_pose['bodies']['candidate']\n",
    "    candidate_face = detected_pose['faces'][0]\n",
    "    candidate_hand = detected_pose['hands']\n",
    "    candidate_body[:,0] = (candidate_body[:,0]-w_min)/(w_max-w_min)\n",
    "    candidate_body[:,1] = (candidate_body[:,1]-h_min)/(h_max-h_min)\n",
    "    candidate_face[:,0] = (candidate_face[:,0]-w_min)/(w_max-w_min)\n",
    "    candidate_face[:,1] = (candidate_face[:,1]-h_min)/(h_max-h_min)\n",
    "    candidate_hand[:,:,0] = (candidate_hand[:,:,0]-w_min)/(w_max-w_min)\n",
    "    candidate_hand[:,:,1] = (candidate_hand[:,:,1]-h_min)/(h_max-h_min)\n",
    "    detected_pose['bodies']['candidate'] = candidate_body\n",
    "    detected_pose['faces'] = candidate_face.reshape(1, candidate_face.shape[0], candidate_face.shape[1])\n",
    "    detected_pose['hands'] = candidate_hand\n",
    "    detected_pose['draw_pose_params'] = draw_pose_params\n",
    "    np.save(save_dir+'/'+str(num)+'.npy', detected_pose)\n",
    "\n",
    "def save_pose_params(detected_poses, pose_params, draw_pose_params, ori_video_path):\n",
    "    save_dir = ori_video_path.replace('video', 'pose/')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    input_list = []\n",
    "    \n",
    "    for i, detected_pose in enumerate(detected_poses):\n",
    "        input_list.append([detected_pose, pose_params, draw_pose_params, save_dir])\n",
    "\n",
    "    pool = ThreadPool(8)\n",
    "    pool.map(save_pose_params_item, input_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return save_dir\n",
    "from torchvision.transforms import functional as F\n",
    "def get_img_pose(\n",
    "        img_path: str, \n",
    "        sample_stride: int=1,\n",
    "        max_frame=None):\n",
    "\n",
    "  # read input img\n",
    "  frame = cv2.imread(img_path)\n",
    "  height, width, _ = frame.shape\n",
    "  short_size = min(height, width)\n",
    "  resize_ratio = max(MAX_SIZE / short_size, 1.0)\n",
    "  frame = cv2.resize(frame, (int(resize_ratio * width), int(resize_ratio * height)))\n",
    "  height, width, _ = frame.shape\n",
    "  detected_poses = [dwprocessor(frame)]\n",
    "  dwprocessor.release_memory()\n",
    "\n",
    "  return detected_poses, height, width, frame\n",
    "\n",
    "def save_aligned_img(ori_frame, video_params, max_size):\n",
    "  h_min_real, h_max_real, w_min_real, w_max_real = video_params\n",
    "  img = ori_frame[h_min_real:h_max_real,w_min_real:w_max_real,:]\n",
    "  img_aligened = resize_and_pad(img, max_size=max_size)\n",
    "  print('aligned img shape:', img_aligened.shape)\n",
    "  save_dir = './assets/refimg_aligned'\n",
    "\n",
    "  os.makedirs(save_dir, exist_ok=True)\n",
    "  save_path = os.path.join(save_dir, 'aligned.png')\n",
    "  cv2.imwrite(save_path, img_aligened)\n",
    "  return save_path\n",
    "\n",
    "detected_poses, height, width, ori_frame = get_img_pose(refimg_path, max_frame=None)\n",
    "res_params = get_pose_params(detected_poses, MAX_SIZE)\n",
    "refimg_aligned_path = save_aligned_img(ori_frame, res_params['video_params'], MAX_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13abf99-6af2-4d57-a348-99020e23b3f1",
   "metadata": {
    "libroFormatter": "formatter-string"
   },
   "source": [
    "## Extract pose from driving video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e61f8-ddd7-48b1-8738-6b404ac7a6dc",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "if using_video_driving:\n",
    "  base_dir = video_dir\n",
    "  tasks = [video_name]\n",
    "  visualization = False\n",
    "  for sub_task in tasks:\n",
    "    ori_list = os.listdir(base_dir+sub_task)\n",
    "    new_dir = base_dir + sub_task+'_24fps'\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    index = 1\n",
    "    for i, mp4_file in enumerate(ori_list):\n",
    "      ori_video_path = base_dir + sub_task+'/'+mp4_file\n",
    "      if ori_video_path[-3:]=='mp4':\n",
    "        try:\n",
    "          # convert to 24fps\n",
    "          ori_video_path_new = ori_video_path.replace(sub_task, sub_task+'_24fps')\n",
    "          if '.MOV' in ori_video_path_new:\n",
    "              ori_video_path_new.replace('.MOV', '.mp4')\n",
    "          convert_fps(ori_video_path, ori_video_path_new)\n",
    "          # extract pose\n",
    "          detected_poses, height, width, ori_frames = get_video_pose(ori_video_path_new, max_frame=None)\n",
    "          # parameterize pose\n",
    "          res_params = get_pose_params(detected_poses, MAX_SIZE)\n",
    "          # save pose to npy\n",
    "          pose_path = save_pose_params(detected_poses, res_params['pose_params'], res_params['draw_pose_params'], ori_video_path)\n",
    "          \n",
    "          index += 1\n",
    "            \n",
    "        except:\n",
    "          print(\"extract crash!\")\n",
    "          continue \n",
    "\n",
    "    print([\"All Finished\", sub_task, start, end])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f8ba95-dc2f-4d85-8834-8259933ca8d5",
   "metadata": {
    "libroFormatter": "formatter-string"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cb9bf-ea25-459c-b7a0-096093e3f038",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "from einops import repeat\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "from decord import VideoReader\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "os.environ['FFMPEG_PATH'] = './ffmpeg-4.4-amd64-static'\n",
    "ffmpeg_path = os.getenv('FFMPEG_PATH')\n",
    "\n",
    "if ffmpeg_path is None:\n",
    "    print(\"please download ffmpeg-static and export to FFMPEG_PATH. \\nFor example: export FFMPEG_PATH=./ffmpeg-4.4-amd64-static\")\n",
    "elif ffmpeg_path not in os.getenv('PATH'):\n",
    "    print(\"add ffmpeg to path\")\n",
    "    os.environ[\"PATH\"] = f\"{ffmpeg_path}:{os.environ['PATH']}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c9eff-2976-4840-9d1d-c86eec06add4",
   "metadata": {
    "libroFormatter": "formatter-string"
   },
   "source": [
    "## argparse使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe46b83e-4e14-446e-bdde-01f663629d30",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config='./configs/prompts/infer.yaml', W=768, H=768, L=240, seed=3407, context_frames=12, context_overlap=3, cfg=2.5, steps=30, sample_rate=16000, fps=24, device='cuda', ref_images_dir='./assets/halfbody_demo/refimag', pose_dir=None, refimg_name='natural_bk_openhand/0035.png', pose_name='01', video_dir='./assets/halfbody_demo/video')\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", type=str, default=\"./configs/prompts/infer.yaml\")\n",
    "parser.add_argument(\"-W\", type=int, default=768)\n",
    "parser.add_argument(\"-H\", type=int, default=768)\n",
    "parser.add_argument(\"-L\", type=int, default=240)\n",
    "parser.add_argument(\"--seed\", type=int, default=3407)\n",
    "\n",
    "parser.add_argument(\"--context_frames\", type=int, default=12)\n",
    "parser.add_argument(\"--context_overlap\", type=int, default=3)\n",
    "\n",
    "parser.add_argument(\"--cfg\", type=float, default=2.5)\n",
    "parser.add_argument(\"--steps\", type=int, default=30)\n",
    "parser.add_argument(\"--sample_rate\", type=int, default=16000)\n",
    "parser.add_argument(\"--fps\", type=int, default=24)\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "parser.add_argument(\"--ref_images_dir\", type=str, default=f'./assets/halfbody_demo/refimag')\n",
    "parser.add_argument(\"--pose_dir\", type=str, default=None)\n",
    "parser.add_argument(\"--refimg_name\", type=str, default='natural_bk_openhand/0035.png')\n",
    "parser.add_argument(\"--pose_name\", type=str, default=\"01\")\n",
    "parser.add_argument(\"--video_dir\", type=str, default=\"./assets/halfbody_demo/video\")\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "print(f'{args}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc9840-8390-4b78-8365-a53db438bcc9",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acb3af19-2df5-4923-b4bf-4402773d864d",
   "metadata": {
    "libroFormatter": "formatter-string"
   },
   "source": [
    "## Animating half-body human video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7dbb28",
   "metadata": {},
   "source": [
    "## 图片、视频、音频展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa341db6-81ff-4cf0-8e89-41e92170a0d9",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Video\n",
    "# 播放视频：\n",
    "# display(Video(filename=save_name + \"_sig.mp4\"))\n",
    "\n",
    "\n",
    "# # # 展示图片：\n",
    "from IPython.display import Image as DisImage\n",
    "# display(Image(filename=\"path/to/image.jpg\"))\n",
    "\n",
    "# ref_img = './zlm-digital-person.git/data/frame/liveV_frames-0-at-3.920.jpg'\n",
    "# display(Image(filename=ref_img))\n",
    "# clip = VideoFileClip('zlm-digital-person.git/data/video/output_video_img_0_text_0.mp4') # v1输出的音频驱动人头视频\n",
    "# display(Video(filename='zlm-digital-person.git/data/video/output_video_img_0_text_0.mp4'))\n",
    "# for fi in clip.iter_frames():\n",
    "#     print('sub frame1: ')\n",
    "#     display(Image(fi))\n",
    "#     break # test \n",
    "\n",
    "# # 播放 音频文件： \n",
    "# from IPython.display import *     \n",
    "# Audio(audio_path, autoplay=True)\n",
    "\n",
    "# # 播放视频文件的方式2： \n",
    "# from IPython.display import HTML\n",
    "# def play_video(video_path):\n",
    "#     # 构建 HTML5 视频标签\n",
    "#     video_html = f\"\"\"\n",
    "#     <video width=\"600\" controls>\n",
    "#         <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "#         您的浏览器不支持视频标签\n",
    "#     </video>\n",
    "#     \"\"\"\n",
    "#     display(HTML(video_html))\n",
    "\n",
    "# # 示例：播放当前目录下的 sample.mp4\n",
    "# play_video(\"outputs/aligned-a-echomimicv2_woman-i0_sig.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
